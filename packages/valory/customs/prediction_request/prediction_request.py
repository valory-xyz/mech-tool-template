# -*- coding: utf-8 -*-
# ------------------------------------------------------------------------------
#
#   Copyright 2023-2024 Valory AG
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
# ------------------------------------------------------------------------------
"""Contains the job definitions"""

from typing import Any, Dict, Optional, Tuple
import requests
import json

DEFAULT_MODEL = "meta-llama/llama-3-8b-instruct:free"


PREDICTION_PROMPT="""
You are an LLM inside a multi-agent system that takes in user questions about whether an event is likely to happen or not.
Answers to those questions should only be Y (for YES) or N (for NO).
You need to provide a YES/NO answer based on your training data.

Only respond with the format below using curly brackets to encapsulate the variables within a json dictionary object and no other text:

"response": "response"

USER_PROMPT:

{user_prompt}
"""


def error_response(msg: str) -> Tuple[str, None, None, None]:
    """Return an error mech response."""
    return msg, None, None, None


def run(**kwargs) -> Tuple[Optional[str], Optional[Dict[str, Any]], Any, Any]:
    """Run the task

    Returns:
    - Response to send to the user
    - [Optional] Prompt sent to the model
    - [Optional] Transaction generated by the tool to be executed by the mech
    - [Optional] Cost calculation object
    """

    # Get the prompt
    prompt = kwargs.get("prompt", None)
    if prompt is None:
        return error_response("No prompt has been specified.")
    prompt = PREDICTION_PROMPT.replace("{user_prompt}", kwargs["prompt"])

    # Get the model
    model = kwargs.get("model", DEFAULT_MODEL)

    # Get the API key
    openrouter_api_key = kwargs.get("api_keys", {}).get("openrouter", None)
    if openrouter_api_key is None:
        return error_response("No openrouter API key has been specified.")

    # Call the LLM
    data = json.dumps({
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ]
    })

    response = requests.post(
        url="https://openrouter.ai/api/v1/chat/completions",
        headers={"Authorization": f"Bearer {openrouter_api_key}"},
        data=data
    )

    result = response.json()["choices"][0]["message"]["content"]

    return result, None, None, None
